# 로컬 Llama3 RAG 챗봇 프로젝트 (Local Llama3 RAG Chatbot Project)

## 프로젝트 설명

이 프로젝트는 Llama3와 RAG(Retrieval Augmented Generation) 아키텍처를 활용하여, **외부 API 의존성 없이 로컬 환경에서 실행 가능한 챗봇**을 구현합니다. 사용자가 제공하는 문서를 기반으로 답변을 생성하는 기능을 포함합니다.

---

## 사용 기술 (Technologies Used)

이 프로젝트는 다음과 같은 주요 기술 스택을 사용합니다:

* **모델:** Llama3 (Ollama를 통해 로컬에서 실행)
* **UI 프레임워크:** Streamlit
* **문서 검색:** RAG(Retrieval Augmented Generation) 기반 검색 엔진
* **구성 방식:** 완전히 로컬 환경에서만 작동하도록 설계

---

## 주요 특징 (Key Features)

* **독립적인 로컬 실행:** 외부 API(예: OpenAI API)에 대한 의존성 없이 전체 시스템이 로컬 환경에서 실행됩니다.
* **통합 아키텍처:** LangChain, Streamlit, Ollama를 유기적으로 통합하여 유연하고 강력한 챗봇 환경을 제공합니다.
* **사용자 문서 기반 RAG:** 사용자가 제공하는 문서를 활용하여 질문에 대한 답변을 생성함으로써, 일반적인 LLM의 한계를 넘어선 정확하고 맞춤화된 응답을 제공합니다.

---

## 설치 및 실행 방법 (Installation & Usage)

*(여기에 프로젝트를 설치하고 실행하는 구체적인 명령어나 절차를 추가해 주세요. 예를 들어 `git clone`, `poetry install`, `poetry run langchain serve`, `streamlit run app.py` 등)*

---

## 라이선스 (License)

*(여기에 프로젝트 라이선스 정보를 추가해 주세요. 예: MIT License)*

---
